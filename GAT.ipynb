{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "GAT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2hip3ng/lihang-code/blob/master/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13YTsLJUkkAH",
        "colab_type": "text"
      },
      "source": [
        "## 挂载云盘"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPz2xoviksWK",
        "colab_type": "code",
        "outputId": "a4a7a3f1-1d50-426b-9362-27c92f2e976a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMGNJNUenckz",
        "colab_type": "code",
        "outputId": "620cd558-7d49-465f-aeee-cd1ffd7c084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/match-gat'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/match-gat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WzS_y_FnNto",
        "colab_type": "code",
        "outputId": "c3a05b13-948f-4608-8f76-b7a5eac318d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/match-gat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctwoCXFZj7ey",
        "colab_type": "text"
      },
      "source": [
        "## 导入相关包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW3tTjRBj7ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import pickle\n",
        "import codecs\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "LayerNorm = torch.nn.LayerNorm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3wZPtHWciZn",
        "colab_type": "text"
      },
      "source": [
        "## 激活函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpQk-0lMhdFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRS4JSUij7e4",
        "colab_type": "text"
      },
      "source": [
        "## 模型配置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O1InjU-j7e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config():\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50000,\n",
        "        embedding_size=300,\n",
        "        hidden_size=300,\n",
        "        num_hidden_layers=5,\n",
        "        num_attention_heads=5,\n",
        "        intermediate_size=2048,\n",
        "        hidden_act=\"relu\",\n",
        "        embedding_dropout_prob=0.2,\n",
        "        num_labels = 3,\n",
        "        hidden_dropout_prob=0.2,\n",
        "        attention_probs_dropout_prob=0.2,\n",
        "        max_position_embeddings_a=32,\n",
        "        max_position_embeddings_b=16,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        norm_eps=1e-12,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_labels = num_labels\n",
        "        self.embedding_dropout_prob = embedding_dropout_prob\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings_a = max_position_embeddings_a\n",
        "        self.max_position_embeddings_b = max_position_embeddings_b\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.norm_eps = norm_eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVKJ4LdZj7eu",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6EOChjVZ3PD",
        "colab_type": "text"
      },
      "source": [
        "### CharacterEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_0wjXrhZ2iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharacterEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Character embedding module.\n",
        "    :param char_embedding_input_dim: The input dimension of character embedding layer.\n",
        "    :param char_embedding_output_dim: The output dimension of character embedding layer.\n",
        "    :param char_conv_filters: The filter size of character convolution layer.\n",
        "    :param char_conv_kernel_size: The kernel size of character convolution layer.\n",
        "    Examples:\n",
        "        >>> import torch\n",
        "        >>> character_embedding = CharacterEmbedding()\n",
        "        >>> x = torch.ones(10, 32, 16, dtype=torch.long)\n",
        "        >>> x.shape\n",
        "        torch.Size([10, 32, 16])\n",
        "        >>> character_embedding(x).shape\n",
        "        torch.Size([10, 32, 100])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        char_embedding_input_dim: int = 100,\n",
        "        char_embedding_output_dim: int = 8,\n",
        "        char_conv_filters: int = 300,\n",
        "        char_conv_kernel_size: int = 5\n",
        "    ):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super().__init__()\n",
        "        self.char_embedding = nn.Embedding(\n",
        "            num_embeddings=char_embedding_input_dim,\n",
        "            embedding_dim=char_embedding_output_dim\n",
        "        )\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=char_embedding_output_dim,\n",
        "            out_channels=char_conv_filters,\n",
        "            kernel_size=char_conv_kernel_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        embed_x = self.char_embedding(x)\n",
        "\n",
        "        batch_size, seq_len, word_len, embed_dim = embed_x.shape\n",
        "\n",
        "        embed_x = embed_x.contiguous().view(-1, word_len, embed_dim)\n",
        "\n",
        "        embed_x = self.conv(embed_x.transpose(1, 2))\n",
        "        embed_x = torch.max(embed_x, dim=-1)[0]\n",
        "\n",
        "        embed_x = embed_x.view(batch_size, seq_len, -1)\n",
        "        return embed_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZjwleZyj7e_",
        "colab_type": "text"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dswTJXFdjfu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\n",
        "        # self.max_position_embeddings = max(config.max_position_embeddings_a, config.max_position_embeddings_b)\n",
        "        # self.position_embeddings = nn.Embedding(self.max_position_embeddings, config.embedding_size)\n",
        "        self.dropout = nn.Dropout(config.embedding_dropout_prob)\n",
        "        \n",
        "        if not os.path.exists('word_embedding_snli.pkl'):\n",
        "            print('load embedding ... ')\n",
        "            with open('snli/vocab.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "            word2id = {}\n",
        "            vocab = []\n",
        "            for (index, line) in enumerate(lines):\n",
        "                word = line.strip()\n",
        "                vocab.append(word)\n",
        "                word2id[word] = index\n",
        "\n",
        "            embedding = np.zeros((config.vocab_size, 300))\n",
        "            tar_count = 0\n",
        "            glove_vocab = {}\n",
        "            with open('glove.840B.300d.txt') as f:\n",
        "                for line in f:\n",
        "                    elems = line.rstrip().split()\n",
        "                    if len(elems) != 300 + 1:\n",
        "                        continue\n",
        "                    token = elems[0]\n",
        "\n",
        "                    # token = token.lower()\n",
        "                    if token in vocab:\n",
        "                        index = vocab.index(token)\n",
        "                        vector = [float(x) for x in elems[1:]]\n",
        "                        embedding[index] = vector\n",
        "                        if token not in glove_vocab.keys():\n",
        "                            tar_count += 1\n",
        "                            glove_vocab[token] = 1\n",
        "                    else:\n",
        "                        token = token.lower()\n",
        "                        if token in vocab and token not in glove_vocab.keys():\n",
        "                            index = vocab.index(token)\n",
        "                            vector = [float(x) for x in elems[1:]]\n",
        "                            embedding[index] = vector\n",
        "                            tar_count += 1\n",
        "                            glove_vocab[token] = 1\n",
        "\n",
        "            print('oov:', len(vocab) - tar_count, ' 比例：', (len(vocab) - tar_count) / len(vocab))\n",
        "            \n",
        "            with open('word_embedding_snli.pkl', 'wb') as f:\n",
        "                pickle.dump(embedding,f)\n",
        "        else:\n",
        "            with open('word_embedding_snli.pkl', 'rb') as f:\n",
        "                embedding = pickle.load(f)\n",
        "        self.word_embeddings.weight.data.copy_(torch.from_numpy(embedding))\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, input_ids, position=None):\n",
        "        word_embeddings = self.word_embeddings(input_ids)\n",
        "        word_embeddings = self.dropout(word_embeddings)    \n",
        "        return word_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM8TrGCag9tn",
        "colab_type": "text"
      },
      "source": [
        "### Self Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfMFgOwsjw3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttLayer(nn.Module):\n",
        "    \"\"\"docstring for GatLayer\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.norm = LayerNorm(config.hidden_size, eps=config.norm_eps)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        # x : batch_size * max_seq * dim\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)  \n",
        "        # new_x_shape: batch_size * max_seq * attention_heads * head_size\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # return shape: batch_size * attention_heads * max_seq * head_size\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        # hidden_states_a: batch_size * max_seq_a * embedding_dim\n",
        "        # hidden_states_b: batch_size * max_seq_b * embedding_dim\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        attention_mask = extended_attention_mask\n",
        "\n",
        "        # Self-Attention\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_a * max_seq_b\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        return context_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OsylmHwhBY0",
        "colab_type": "text"
      },
      "source": [
        "### FeedForward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGjJnk-nmAgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.norm = LayerNorm(config.hidden_size, eps=config.norm_eps)\n",
        "    \n",
        "    def forward(self, hidden_states):\n",
        "        output = self.dense_1(hidden_states)\n",
        "        # x = nn.functional.relu(hidden_states)\n",
        "        output = gelu(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.dense_2(output)\n",
        "        output = self.norm(hidden_states + output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klAFA3tkhEUI",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBE79t9Bmj0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.selfattlayer = SelfAttLayer(config)\n",
        "        self.feedforward = FeedForward(config)\n",
        "        self.norm = LayerNorm(config.hidden_size, eps=config.norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        output = self.selfattlayer(hidden_states, attention_mask)\n",
        "        output = self.dropout(output)\n",
        "        output = self.norm(output + hidden_states)\n",
        "        output = self.feedforward(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKOHdnCThD2F",
        "colab_type": "text"
      },
      "source": [
        "### CrossAttLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMl46EKSoDAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossAttLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.dense = nn.Linear(config.hidden_size * 4, config.hidden_size)    ### 87.9\n",
        "        # self.dense = nn.Linear(config.hidden_size * 2, config.hidden_size)    ### 88.0 20epoch\n",
        "        self.norm = LayerNorm(config.hidden_size, eps=config.norm_eps)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        # x : batch_size * max_seq * dim\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)  \n",
        "        # new_x_shape: batch_size * max_seq * attention_heads * head_size\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # return shape: batch_size * attention_heads * max_seq * head_size\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b):\n",
        "        # hidden_states_a: batch_size * max_seq_a * embedding_dim\n",
        "        # hidden_states_b: batch_size * max_seq_b * embedding_dim\n",
        "\n",
        "        mixed_query_layer_a = hidden_states_a\n",
        "        mixed_key_layer_a = hidden_states_a\n",
        "        mixed_value_layer_a = hidden_states_a\n",
        "\n",
        "        query_layer_a = self.transpose_for_scores(mixed_query_layer_a)\n",
        "        key_layer_a = self.transpose_for_scores(mixed_key_layer_a)\n",
        "        value_layer_a = self.transpose_for_scores(mixed_value_layer_a)\n",
        "\n",
        "        mixed_query_layer_b = hidden_states_b\n",
        "        mixed_key_layer_b = hidden_states_b\n",
        "        mixed_value_layer_b = hidden_states_b\n",
        "\n",
        "        query_layer_b = self.transpose_for_scores(mixed_query_layer_b)\n",
        "        key_layer_b = self.transpose_for_scores(mixed_key_layer_b)\n",
        "        value_layer_b = self.transpose_for_scores(mixed_value_layer_b)\n",
        "\n",
        "        extended_attention_mask_a = attention_mask_a[:, None, None, :]\n",
        "        extended_attention_mask_a = (1.0 - extended_attention_mask_a) * -10000.0\n",
        "        attention_mask_a = extended_attention_mask_a\n",
        "\n",
        "        extended_attention_mask_b = attention_mask_b[:, None, None, :]\n",
        "        extended_attention_mask_b = (1.0 - extended_attention_mask_b) * -10000.0\n",
        "        attention_mask_b = extended_attention_mask_b\n",
        "\n",
        "\n",
        "        attention_scores_a2b = torch.matmul(query_layer_a, key_layer_b.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_a * max_seq_b\n",
        "        attention_scores_a2b = attention_scores_a2b / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_a2b = attention_scores_a2b + attention_mask_b\n",
        "        attention_probs_a2b = nn.Softmax(dim=-1)(attention_scores_a2b)\n",
        "        attention_probs_a2b = self.dropout(attention_probs_a2b)\n",
        "        context_layer_a2b = torch.matmul(attention_probs_a2b, value_layer_b)\n",
        "        context_layer_a2b = context_layer_a2b.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_a2b = context_layer_a2b.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_a2b = context_layer_a2b.view(*new_context_layer_shape_a2b)\n",
        "\n",
        "\n",
        "        \n",
        "        attention_scores_b2a = torch.matmul(query_layer_b, key_layer_a.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_b * max_seq_a\n",
        "        attention_scores_b2a = attention_scores_b2a / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_b2a = attention_scores_b2a + attention_mask_a\n",
        "        attention_probs_b2a = nn.Softmax(dim=-1)(attention_scores_b2a)\n",
        "        attention_probs_b2a = self.dropout(attention_probs_b2a)\n",
        "        context_layer_b2a = torch.matmul(attention_probs_b2a, value_layer_a)\n",
        "        context_layer_b2a = context_layer_b2a.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_b2a = context_layer_b2a.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_b2a = context_layer_b2a.view(*new_context_layer_shape_b2a)\n",
        "\n",
        "        context_layer_a = torch.cat([hidden_states_a, context_layer_a2b, \n",
        "                    hidden_states_a - context_layer_a2b, hidden_states_a * context_layer_a2b], -1)  ## 87.9\n",
        "\n",
        "        context_layer_b = torch.cat([hidden_states_b, context_layer_b2a, \n",
        "                    hidden_states_b - context_layer_b2a, hidden_states_b * context_layer_b2a], -1) ## 87.9\n",
        "        \n",
        "        # context_layer_a = torch.cat([hidden_states_a, context_layer_a2b], -1)     ### 88.0 20epoch\n",
        "\n",
        "        # context_layer_b = torch.cat([hidden_states_b, context_layer_b2a], -1)\n",
        "        \n",
        "\n",
        "        context_layer_a = self.dense(context_layer_a)\n",
        "        context_layer_a = gelu(context_layer_a)\n",
        "\n",
        "        context_layer_b = self.dense(context_layer_b)\n",
        "        context_layer_b = gelu(context_layer_b)\n",
        "\n",
        "        context_layer_a = self.dropout(context_layer_a)\n",
        "        context_layer_b = self.dropout(context_layer_b)\n",
        "\n",
        "        context_layer_a = self.norm(hidden_states_a + context_layer_a)\n",
        "        context_layer_b = self.norm(hidden_states_b + context_layer_b)\n",
        "        \n",
        "        outputs = (context_layer_a, context_layer_b)\n",
        "        \n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzxbeuS7hqk8",
        "colab_type": "text"
      },
      "source": [
        "### Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkbI99Qrpg2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(config)\n",
        "        self.crossattlayer = CrossAttLayer(config)\n",
        "        \n",
        "        self.norm = LayerNorm(config.hidden_size, eps=config.norm_eps)\n",
        "    \n",
        "    def forward(self, hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b):\n",
        "        out_a = self.encoder(hidden_states_a, attention_mask_a)\n",
        "        out_b = self.encoder(hidden_states_b, attention_mask_b)\n",
        "        out_a, out_b = self.crossattlayer(out_a, out_b, attention_mask_a, attention_mask_b)\n",
        "        return out_a, out_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbHP9d0Nf6Kv",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHcuzJahf5ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        num_filters = 256\n",
        "        filter_sizes = (2, 3, 4)\n",
        "        embed = 300\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv2d(1, num_filters, (k, embed)) for k in filter_sizes])\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.relu(conv(x)).squeeze(3)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.unsqueeze(1)   # b * 1 * s * e\n",
        "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
        "        # out = self.dropout(0.2)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30SL0yUkhzRX",
        "colab_type": "text"
      },
      "source": [
        "### Pooling & Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCmM4Kajh2Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = CNN()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.cnn(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "        '''\n",
        "        x_avg = torch.sum(x * mask.unsqueeze(1).transpose(2, 1), dim=1)\\\n",
        "                            / torch.sum(mask, dim=1, keepdim=True)\n",
        "        # max pooling\n",
        "        # print('mask:', mask)\n",
        "        extended_mask = mask[:, :, None]\n",
        "        extended_mask = (1.0 - extended_mask) * (-100000)\n",
        "        mask = extended_mask\n",
        "        # print('extended_mask:', mask)\n",
        "        x = x + mask\n",
        "        # return torch.cat([x.max(dim=1)[0], x_avg], -1)\n",
        "        return x.max(dim=1)[0]\n",
        "        '''\n",
        "    \n",
        "class Prediction(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dense_1 = nn.Linear(256 * 3 * 5, config.hidden_size * 2)\n",
        "        # self.dense_1 = nn.Linear(config.hidden_size * 5, config.hidden_size * 2)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.dense_2 = nn.Linear(config.hidden_size * 2, config.num_labels)\n",
        "    def forward(self, a, b):\n",
        "        outputs = torch.cat([a, b, a - b, a * b, torch.abs(a-b)], dim=-1)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.dense_1(outputs)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V75WGntIu7RA",
        "colab_type": "text"
      },
      "source": [
        "### Focal Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7liohSvAat",
        "colab_type": "text"
      },
      "source": [
        " $ Loss(x, class) = - \\alpha (1-softmax(x)[class])^ {gamma} \\log(softmax(x)[class]) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLuuJTtYu6Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
        "        Focal Loss for Dense Object Detection.\n",
        "\n",
        "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^ {gamma} \\log(softmax(x)[class])\n",
        "\n",
        "        The losses are averaged across observations for each minibatch.\n",
        "\n",
        "        Args:\n",
        "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
        "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
        "                                   putting more focus on hard, misclassiﬁed examples\n",
        "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
        "                                However, if the field size_average is set to False, the losses are\n",
        "                                instead summed for each minibatch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, class_num=3, alpha=None, gamma=2, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if alpha is None:\n",
        "            self.alpha = Variable(torch.ones(class_num, 1))\n",
        "        else:\n",
        "            if isinstance(alpha, Variable):\n",
        "                self.alpha = alpha\n",
        "            else:\n",
        "                self.alpha = Variable(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.class_num = class_num\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        N = inputs.size(0)\n",
        "        C = inputs.size(1)\n",
        "        P = F.softmax(inputs)\n",
        "\n",
        "        class_mask = inputs.data.new(N, C).fill_(0)\n",
        "        class_mask = Variable(class_mask)\n",
        "        ids = targets.view(-1, 1)\n",
        "        class_mask.scatter_(1, ids.data, 1.)\n",
        "        #print(class_mask)\n",
        "\n",
        "\n",
        "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
        "            self.alpha = self.alpha.cuda()\n",
        "        alpha = self.alpha[ids.data.view(-1)]\n",
        "\n",
        "        probs = (P*class_mask).sum(1).view(-1,1)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        #print('probs size= {}'.format(probs.size()))\n",
        "        #print(probs)\n",
        "\n",
        "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
        "        #print('-----bacth_loss------')\n",
        "        #print(batch_loss)\n",
        "\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrwPI04fi2va",
        "colab_type": "text"
      },
      "source": [
        "### MatchModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tlikqFBj7fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embedding = EmbeddingLayer(config)\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.pooling = Pooling()\n",
        "        self.prediction = Prediction(config)\n",
        "\n",
        "        # self.init_weights()\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "        # self.loss_fct = FocalLoss()\n",
        "         \n",
        "        \n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input_ids_a, input_ids_b, attention_mask_a, attention_mask_b, labels):\n",
        "        hidden_states_a = self.embedding(input_ids_a)\n",
        "        hidden_states_b = self.embedding(input_ids_b)\n",
        "        for i, layer in enumerate(self.blocks):\n",
        "            hidden_states_a, hidden_states_b = layer(hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b)\n",
        "\n",
        "        # print('hidden_states_a:', hidden_states_a, hidden_states_a.shape)\n",
        "        # print('hidden_states_b:', hidden_states_b, hidden_states_b.shape)\n",
        "        # os._exit(1)\n",
        "\n",
        "        outputs_a = self.pooling(hidden_states_a, attention_mask_a)\n",
        "        outputs_b = self.pooling(hidden_states_b, attention_mask_b)\n",
        "\n",
        "        outputs = self.prediction(outputs_a, outputs_b)\n",
        "        # loss = self.loss_fct(outputs.view(-1, config.num_labels), labels.view(-1))\n",
        "        loss = self.loss_fct(outputs, labels)\n",
        "        # print(loss)\n",
        "        # print(outputs)\n",
        "        outputs = (loss, outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    \"\"\"\n",
        "    def init_weights(self, module):\n",
        "        # Initialize the weights \n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, norm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkTRyz08j7fF",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58j1yfAwj7fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def sentence2ids(args, sentence, word2id):\n",
        "    if args.do_lower_case:\n",
        "        sentence = sentence.lower()\n",
        "    ids = []\n",
        "    for word in  sentence.strip().split():\n",
        "        if word not in word2id.keys():\n",
        "            ids.append(word2id['<UNK>'])\n",
        "        else:\n",
        "            ids.append(word2id[word])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def load_vocab(args):\n",
        "    import codecs\n",
        "    vocab_path = os.path.join(args.data_dir, 'vocab.txt')\n",
        "\n",
        "    if not os.path.exists(vocab_path):\n",
        "        vocab = Counter()\n",
        "        files = os.listdir(args.data_dir)\n",
        "        for file in files:\n",
        "            if not os.path.isdir(file) and file != '.DS_Store':\n",
        "                # print('file:', file)\n",
        "                f = codecs.open(os.path.join(args.data_dir, file), 'r')\n",
        "                for line in f.readlines():\n",
        "                    text_a, text_b, label = line.strip().split('\\t')\n",
        "                    if args.do_lower_case:\n",
        "                        text_a = text_a.lower()\n",
        "                        text_b = text_b.lower()\n",
        "                    vocab.update(text_a.split())\n",
        "                    vocab.update(text_b.split())\n",
        "                f.close()\n",
        "        f = codecs.open(os.path.join(args.data_dir+'/vocab.txt'), 'w')\n",
        "        vocab = vocab.items()\n",
        "        vocab = sorted(vocab, key=lambda x:x[1], reverse=True)\n",
        "        f.write('<PAD>\\n<UNK>\\n')\n",
        "        for _ in vocab:\n",
        "            f.write(_[0] + '\\n')   \n",
        "\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    word2id = {}\n",
        "    vocab = []\n",
        "    for (index, line) in enumerate(lines):\n",
        "        word = line.strip()\n",
        "        vocab.append(word)\n",
        "        word2id[word] = index\n",
        "\n",
        "    return vocab, word2id\n",
        "\n",
        "def load_dataset(args, word2id, data_type):\n",
        "    data_path = os.path.join(args.data_dir, data_type+'.txt')\n",
        "\n",
        "    # Read Data\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    examples = []\n",
        "    for (i, line) in enumerate(lines):\n",
        "        if len(line.strip().split('\\t')) == 3:\n",
        "            text_a, text_b, label = line.strip().split('\\t')\n",
        "        examples.append((text_a, text_b, label))\n",
        "\n",
        "    # Convert to features\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        len_examples = len(examples)\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d/%d\" % (ex_index, len_examples))\n",
        "\n",
        "        input_ids_a = sentence2ids(args, example[0], word2id)\n",
        "        attention_mask_a = [1] * len(input_ids_a)\n",
        "        padding_length_a = args.max_seq_length_a - len(input_ids_a)\n",
        "        input_ids_a = input_ids_a + ([0] * padding_length_a)\n",
        "        attention_mask_a = attention_mask_a + ([0] * padding_length_a)\n",
        "\n",
        "        input_ids_b = sentence2ids(args, example[1], word2id)\n",
        "        attention_mask_b = [1] * len(input_ids_b)\n",
        "        padding_length_b = args.max_seq_length_b - len(input_ids_b)\n",
        "        input_ids_b = input_ids_b + ([0] * padding_length_b)\n",
        "        attention_mask_b = attention_mask_b + ([0] * padding_length_b)\n",
        "\n",
        "        if example[2] not in ['0', '1', '2']:\n",
        "            # print(example[0], example[1], example[2])\n",
        "            continue\n",
        "        label = int(example[2])\n",
        "\n",
        "        input_ids_a = input_ids_a[:args.max_seq_length_a]\n",
        "        attention_mask_a = attention_mask_a[:args.max_seq_length_a]\n",
        "\n",
        "        input_ids_b = input_ids_b[:args.max_seq_length_b]\n",
        "        attention_mask_b = attention_mask_b[:args.max_seq_length_b]\n",
        "        features.append((input_ids_a, attention_mask_a, input_ids_b, attention_mask_b, label))\n",
        "        \n",
        "        # if ex_index == 1:\n",
        "        #     print('input_sentence_a: ', example[0])\n",
        "        #     print('input_ids_a: ', input_ids_a)\n",
        "        #     print('attention_mask_a: ', attention_mask_a)\n",
        "            \n",
        "        #     print('input_sentence_b: ', example[1])\n",
        "        #     print('input_ids_b: ', input_ids_b)\n",
        "        #     print('attention_mask_b: ', attention_mask_b)\n",
        "            \n",
        "        #     print('input_label: ', example[2])\n",
        "        #     print('label: ', label)\n",
        "    \n",
        "    all_input_ids_a = torch.tensor([f[0] for f in features], dtype=torch.long)\n",
        "    all_attention_mask_a = torch.tensor([f[1] for f in features], dtype=torch.long)\n",
        "    all_input_ids_b = torch.tensor([f[2] for f in features], dtype=torch.long)\n",
        "    all_attention_mask_b = torch.tensor([f[3] for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f[4] for f in features], dtype=torch.long)\n",
        "\n",
        "    # print('1:', len(all_labels))\n",
        "    dataset = TensorDataset(all_input_ids_a, all_attention_mask_a, all_input_ids_b, all_attention_mask_b, all_labels)\n",
        "    # print('return dataset')\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54j7i54j7fL",
        "colab_type": "text"
      },
      "source": [
        "## 训练 评估  测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifpn95zMj7fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer, word2id):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        elif isinstance(module, LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    \"\"\"\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "    \"\"\"\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "    \"\"\"\n",
        "    \n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    \"\"\"\n",
        "    # optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    \n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps,\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    # print('cnm')\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    # print('nmsl')\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    \"\"\"\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        # set global_step to gobal_step of last saved checkpoint from model path\n",
        "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
        "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "    \"\"\"\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    # print('model zero grad start')\n",
        "    model.zero_grad()\n",
        "#     train_iterator = trange(\n",
        "#         epochs_trained, int(args.num_train_epochs), desc=\"Epoch\",\n",
        "#     )\n",
        "    # print('model zero grad end')\n",
        "    train_iterator = range(\n",
        "        epochs_trained, int(args.num_train_epochs)\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproductibility\n",
        "    \n",
        "    loss_show = []\n",
        "    # print(' start training....')\n",
        "    for epoch, _ in enumerate(train_iterator):\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        epoch_iterator = train_dataloader\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\"input_ids_a\": batch[0], \"attention_mask_a\": batch[1], \n",
        "                \"input_ids_b\": batch[2], \"attention_mask_b\": batch[3],\"labels\": batch[4]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # outputs = model(batch[0], batch[1], batch[2], batch[3], batch[4])\n",
        "            loss = outputs[0]  # model outputs are always tuple \n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            loss_show.append(loss.item())\n",
        "            if(step + 1) % 100 == 0:\n",
        "                print('epochs:', epoch, 'train step:', step, 'total step:', len(epoch_iterator), 'loss:', loss.item())\n",
        "            \n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                # torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                # scheduler.step()  # Update learning rate schedule\n",
        "                # print(\"Decaying learning rate to %g\" % scheduler.get_lr()[0])\n",
        "\n",
        "                base_ratio = args.min_learning_rate / args.learning_rate\n",
        "                if global_step < args.warmup_steps:\n",
        "                    ratio = base_ratio + (1. - base_ratio) / max(1., args.warmup_steps) * global_step\n",
        "                else:\n",
        "                    ratio = max(base_ratio, args.lr_decay_rate ** math.floor((global_step - args.warmup_steps) /\n",
        "                                                                                args.lr_decay_steps))\n",
        "                optimizer.param_groups[0]['lr'] = args.learning_rate * ratio\n",
        "\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "            \n",
        "\n",
        "        logger.info(\" train average loss = %s\", epoch_loss / step)\n",
        "        \n",
        "\n",
        "        dev_dataset = load_dataset(args, word2id, 'test')\n",
        "        f1, preds = evaluate(args, dev_dataset, model, tokenizer, word2id)\n",
        "\n",
        "        f = codecs.open('snli/test.txt', 'r')\n",
        "        f_out = codecs.open('snli_bad_case/bad_case_'+str(epoch) +'.txt', 'w')\n",
        "        lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            if int(line.strip()[-1]) != preds[i]:\n",
        "                f_out.write(line.strip() + '\\t' + str(preds[i]) + '\\n')\n",
        "        print('write bad case!!!')\n",
        "        \n",
        "    # show loss pic\n",
        "    x = []\n",
        "    y = loss_show\n",
        "    for i in range(len(y)):\n",
        "        x.append(i)\n",
        "    \n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.pyplot import MultipleLocator\n",
        "    #从pyplot导入MultipleLocator类，这个类用于设置刻度间隔\n",
        "\n",
        "    %matplotlib inline\n",
        "    plt.plot(x, y)\n",
        "    plt.title('Loss Change')\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "\n",
        "#     x_major_locator=MultipleLocator(0.05)\n",
        "#     #把x轴的刻度间隔设置为0.05，并存在变量里\n",
        "#     y_major_locator=MultipleLocator(0.1)\n",
        "#     #把y轴的刻度间隔设置为0.1，并存在变量里\n",
        "#     ax=plt.gca()\n",
        "#     #ax为两条坐标轴的实例\n",
        "#     ax.xaxis.set_major_locator(x_major_locator)\n",
        "#     #把x轴的主刻度设置为0.05的倍数\n",
        "#     ax.yaxis.set_major_locator(y_major_locator)\n",
        "#     #把y轴的主刻度设置为0.1的倍数\n",
        "#     plt.xlim(0, 1)\n",
        "#     #把x轴的刻度范围设置为0到1\n",
        "#     plt.ylim(0, 5)\n",
        "    #把y轴的刻度范围设置为0到5\n",
        "\n",
        "    # plt.savefig('CrossEntropyLoss.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, eval_dataset, model, tokenizer, word2id):\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # multi-gpu eval\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    # for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\"input_ids_a\": batch[0], \"attention_mask_a\": batch[1], \n",
        "            \"input_ids_b\": batch[2], \"attention_mask_b\": batch[3],\"labels\": batch[4]}\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    print(preds[:20])\n",
        "\n",
        "\n",
        "    # result = f1_score(out_label_ids, preds)\n",
        "    result = accuracy_score(out_label_ids, preds)\n",
        "    logger.info(\"eval average loss = %s, accuracy_score = %s\", eval_loss, result)\n",
        "        \n",
        "\n",
        "    # output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    # with open(output_eval_file, \"w\") as writer:\n",
        "    #     logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "    #     for key in sorted(result.keys()):\n",
        "    #         logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "    #         writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    return result, list(preds)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qovBLLUcj7fR",
        "colab_type": "text"
      },
      "source": [
        "## 运行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvbOhslRj7fS",
        "colab_type": "code",
        "outputId": "9cd0570c-f01d-4f75-800d-8a451da7077a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--data_dir\", default='snli', type=str)\n",
        "    parser.add_argument(\"--output_dir\", default='output', type=str)\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\"--max_seq_length_a\", default=32, type=int)\n",
        "    parser.add_argument(\"--max_seq_length_b\", default=32, type=int)\n",
        "    parser.add_argument(\"--num_train_epochs\", default=25, type=float)\n",
        "    parser.add_argument(\"--do_train\", default=True, type=bool)\n",
        "    parser.add_argument(\"--do_test\", default=True, type=bool)\n",
        "    parser.add_argument(\"--do_lower_case\", default=True, type=bool)\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=512, type=int)\n",
        "    parser.add_argument(\"--per_gpu_eval_batch_size\", default=512, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", default=1e-4, type=float)\n",
        "    parser.add_argument(\"--min_learning_rate\", default=6e-5, type=float)\n",
        "    parser.add_argument(\"--lr_decay_rate\", default=0.95, type=float)\n",
        "    parser.add_argument(\"--lr_decay_steps\", default=10000, type=float)\n",
        "    parser.add_argument(\"--weight_decay\", default=0.01, type=float)\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float)\n",
        "    parser.add_argument(\"--no_cuda\", default=False, type=bool)\n",
        "    parser.add_argument(\"--seed\", default=42, type=int)\n",
        "    \n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",)\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",)\n",
        "    parser.add_argument(\"--eval_all_checkpoints\", action=\"store_true\", help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",)\n",
        "    \n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # Setup CUDA, GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,)\n",
        "    logger.warning(\"device: %s, n_gpu: %s,\", device, args.n_gpu)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "    \n",
        "    # Set Label\n",
        "    label_list = ['0', '1', '2']\n",
        "    \n",
        "    # Set Vocab\n",
        "    vocab, word2id = load_vocab(args)\n",
        "    \n",
        "    # Build Model\n",
        "    model = MatchModel(config())\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training parameters %s\", args)\n",
        "    # print('do train')\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_dataset(args, word2id, 'train')\n",
        "        # print('load dataset finish')\n",
        "        global_step, tr_loss = train(args, train_dataset, model, vocab, word2id)\n",
        "        logger.info(\"global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "        \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    if args.do_train:\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(args.output_dir):\n",
        "            os.makedirs(args.output_dir)\n",
        "    \n",
        "\n",
        "        \n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = model_class.from_pretrained(args.output_dir)\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "        \n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    \n",
        "    if args.do_eval:\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = model_class.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "    \n",
        "\n",
        "    return results\n",
        "\n",
        "    \"\"\"\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/21/2020 16:23:10 - WARNING - __main__ -   device: cuda, n_gpu: 1,\n",
            "04/21/2020 16:23:21 - INFO - __main__ -   Training parameters Namespace(adam_epsilon=1e-08, data_dir='snli', device=device(type='cuda'), do_lower_case=True, do_test=True, do_train=True, eval_all_checkpoints=False, gradient_accumulation_steps=1, learning_rate=0.0001, logging_steps=500, lr_decay_rate=0.95, lr_decay_steps=10000, max_grad_norm=1.0, max_seq_length_a=32, max_seq_length_b=32, max_steps=-1, min_learning_rate=6e-05, n_gpu=1, no_cuda=False, num_train_epochs=25, output_dir='output', per_gpu_eval_batch_size=512, per_gpu_train_batch_size=512, save_steps=500, seed=42, warmup_steps=0, weight_decay=0.01)\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 0/549367\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 10000/549367\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 20000/549367\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 30000/549367\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 40000/549367\n",
            "04/21/2020 16:23:23 - INFO - __main__ -   Writing example 50000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 60000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 70000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 80000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 90000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 100000/549367\n",
            "04/21/2020 16:23:24 - INFO - __main__ -   Writing example 110000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 120000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 130000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 140000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 150000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 160000/549367\n",
            "04/21/2020 16:23:25 - INFO - __main__ -   Writing example 170000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 180000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 190000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 200000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 210000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 220000/549367\n",
            "04/21/2020 16:23:26 - INFO - __main__ -   Writing example 230000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 240000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 250000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 260000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 270000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 280000/549367\n",
            "04/21/2020 16:23:27 - INFO - __main__ -   Writing example 290000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 300000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 310000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 320000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 330000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 340000/549367\n",
            "04/21/2020 16:23:28 - INFO - __main__ -   Writing example 350000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 360000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 370000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 380000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 390000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 400000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 410000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 420000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 430000/549367\n",
            "04/21/2020 16:23:29 - INFO - __main__ -   Writing example 440000/549367\n",
            "04/21/2020 16:23:30 - INFO - __main__ -   Writing example 450000/549367\n",
            "04/21/2020 16:23:30 - INFO - __main__ -   Writing example 460000/549367\n",
            "04/21/2020 16:23:30 - INFO - __main__ -   Writing example 470000/549367\n",
            "04/21/2020 16:23:30 - INFO - __main__ -   Writing example 480000/549367\n",
            "04/21/2020 16:23:30 - INFO - __main__ -   Writing example 490000/549367\n",
            "04/21/2020 16:23:31 - INFO - __main__ -   Writing example 500000/549367\n",
            "04/21/2020 16:23:31 - INFO - __main__ -   Writing example 510000/549367\n",
            "04/21/2020 16:23:31 - INFO - __main__ -   Writing example 520000/549367\n",
            "04/21/2020 16:23:31 - INFO - __main__ -   Writing example 530000/549367\n",
            "04/21/2020 16:23:31 - INFO - __main__ -   Writing example 540000/549367\n",
            "04/21/2020 16:23:33 - INFO - __main__ -   ***** Running training *****\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Num examples = 549367\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Num Epochs = 25\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Instantaneous batch size per GPU = 512\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Total train batch size = 512\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/21/2020 16:23:33 - INFO - __main__ -     Total optimization steps = 26825\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epochs: 0 train step: 99 total step: 1073 loss: 0.8616246581077576\n",
            "epochs: 0 train step: 199 total step: 1073 loss: 0.6995905637741089\n",
            "epochs: 0 train step: 299 total step: 1073 loss: 0.6369693279266357\n",
            "epochs: 0 train step: 399 total step: 1073 loss: 0.619914174079895\n",
            "epochs: 0 train step: 499 total step: 1073 loss: 0.5312556624412537\n",
            "epochs: 0 train step: 599 total step: 1073 loss: 0.5963566899299622\n",
            "epochs: 0 train step: 699 total step: 1073 loss: 0.6133249402046204\n",
            "epochs: 0 train step: 799 total step: 1073 loss: 0.5435006618499756\n",
            "epochs: 0 train step: 899 total step: 1073 loss: 0.5709443092346191\n",
            "epochs: 0 train step: 999 total step: 1073 loss: 0.5392552614212036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "04/21/2020 16:34:03 - INFO - __main__ -    train average loss = 0.6469453636056451\n",
            "04/21/2020 16:34:03 - INFO - __main__ -   Writing example 0/9824\n",
            "04/21/2020 16:34:03 - INFO - __main__ -   ***** Running evaluation *****\n",
            "04/21/2020 16:34:03 - INFO - __main__ -     Num examples = 9824\n",
            "04/21/2020 16:34:03 - INFO - __main__ -     Batch size = 512\n",
            "04/21/2020 16:34:07 - INFO - __main__ -   eval average loss = 0.4546026736497879, accuracy_score = 0.8254275244299675\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2 0 2 1 1 1 0 1 0 1 0 1 1 0 2 2 0 1 1 2]\n",
            "write bad case!!!\n",
            "epochs: 1 train step: 99 total step: 1073 loss: 0.4869741201400757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1MynM5Tkz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO8TIJG6c-f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty_X4ZSQc-ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AKLkcVkc-lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6BiHGlLdAi",
        "colab_type": "text"
      },
      "source": [
        "一些想法:\n",
        "1. bad case 里面 一些由于一个词不一样，导致label区别大；交互层不做a-b, a * b; 最后一层对每个词提取的交互特征结合源该词语义特征，进行一个线性层提取；\n",
        "最后pooling层换textcnn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wEHq0RhoYVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9iGrtg6oYYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_maX-YoYa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpP-xBbsoYfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2fHLYa9oYiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIk1Gci2oYkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTJaoTeyoYm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFbtC8P_oYpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZh2YpjvoYuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYc1lQRloYxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU_4kr-IoYz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxE_45O6oY2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQF9JPuqoYrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti3EUABeoYdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgTz2TdLc-nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg43YAiSc-qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28PQHECGc-si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\n",
        "        # self.max_position_embeddings = max(config.max_position_embeddings_a, config.max_position_embeddings_b)\n",
        "        # self.position_embeddings = nn.Embedding(self.max_position_embeddings, config.embedding_size)\n",
        "        # self.norm = MatchGatNorm(config.embedding_size, eps=config.norm_eps)\n",
        "        self.dropout = nn.Dropout(config.embedding_dropout_prob)\n",
        "        # self.lstm = torch.nn.LSTM(config.embedding_size, config.embedding_size, 1, batch_first=True, dropout=0.2)\n",
        "\n",
        "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # h_0 = Variable(torch.randn(1, batch_size, 512))\n",
        "        # c_0 = Variable(torch.randn(1, batch_size, 512))\n",
        "        # h_0 = h_0.to(device)\n",
        "        # c_0 = c_0.to(device)\n",
        "\n",
        "        if not os.path.exists('word_embedding_snli.pkl'):\n",
        "            print('load embedding ... ')\n",
        "            with open('snli/vocab.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "            word2id = {}\n",
        "            vocab = []\n",
        "            for (index, line) in enumerate(lines):\n",
        "                word = line.strip()\n",
        "                vocab.append(word)\n",
        "                word2id[word] = index\n",
        "\n",
        "            embedding = np.zeros((config.vocab_size, 300))\n",
        "            tar_count = 0\n",
        "            glove_vocab = {}\n",
        "            with open('glove.840B.300d.txt') as f:\n",
        "                for line in f:\n",
        "                    elems = line.rstrip().split()\n",
        "                    if len(elems) != 300 + 1:\n",
        "                        continue\n",
        "                    token = elems[0]\n",
        "\n",
        "                    # token = token.lower()\n",
        "                    if token in vocab:\n",
        "                        index = vocab.index(token)\n",
        "                        vector = [float(x) for x in elems[1:]]\n",
        "                        embedding[index] = vector\n",
        "                        if token not in glove_vocab.keys():\n",
        "                            tar_count += 1\n",
        "                            glove_vocab[token] = 1\n",
        "                    else:\n",
        "                        token = token.lower()\n",
        "                        if token in vocab and token not in glove_vocab.keys():\n",
        "                            index = vocab.index(token)\n",
        "                            vector = [float(x) for x in elems[1:]]\n",
        "                            embedding[index] = vector\n",
        "                            tar_count += 1\n",
        "                            glove_vocab[token] = 1\n",
        "\n",
        "            print('oov:', len(vocab) - tar_count, ' 比例：', (len(vocab) - tar_count) / len(vocab))\n",
        "            \n",
        "            with open('word_embedding_snli.pkl', 'wb') as f:\n",
        "                pickle.dump(embedding,f)\n",
        "        else:\n",
        "            with open('word_embedding_snli.pkl', 'rb') as f:\n",
        "                embedding = pickle.load(f)\n",
        "        self.word_embeddings.weight.data.copy_(torch.from_numpy(embedding))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids_a, input_ids_b, position_a=None, position_b=None):\n",
        "        \"\"\"\n",
        "        input_shape_a = input_ids_a.size()\n",
        "        input_shape_b = input_ids_b.size()\n",
        "        seq_length_a = input_shape_a[1]\n",
        "        seq_length_b = input_shape_b[1]\n",
        "\n",
        "        device = input_ids_a.device\n",
        "\n",
        "        batch_size = input_shape_a[0]\n",
        "\n",
        "        h_0 = Variable(torch.randn(1, batch_size, 512))\n",
        "        c_0 = Variable(torch.randn(1, batch_size, 512))\n",
        "        h_0 = h_0.to(device)\n",
        "        c_0 = c_0.to(device)\n",
        "\n",
        "        input_a_word_embeddings = self.word_embeddings(input_ids_a)\n",
        "        input_b_word_embeddings = self.word_embeddings(input_ids_b)\n",
        "\n",
        "        if position_a:\n",
        "            input_a_position_embeddings = self.position_embeddings(position_a)\n",
        "            input_b_position_embeddings = self.position_embeddings(position_b)\n",
        "\n",
        "            a_embeddings = input_a_word_embeddings + input_a_position_embeddings\n",
        "            a_embeddings = self.norm(a_embeddings)\n",
        "            a_embeddings = self.dropout(a_embeddings)\n",
        "\n",
        "            b_embeddings = input_b_word_embeddings + input_b_position_embeddings\n",
        "            b_embeddings = self.norm(b_embeddings)\n",
        "            b_embeddings = self.dropout(b_embeddings)\n",
        "        else:\n",
        "            a_embeddings = input_a_word_embeddings\n",
        "\n",
        "            # a_embeddings = self.norm(a_embeddings)\n",
        "            a_embeddings = self.dropout(a_embeddings)\n",
        "\n",
        "            b_embeddings = input_b_word_embeddings\n",
        "            # b_embeddings = self.norm(b_embeddings)\n",
        "            b_embeddings = self.dropout(b_embeddings)\n",
        "            \n",
        "            \n",
        "            # # print('random h, c')\n",
        "            # a_embeddings, (h_x, c_x) = self.lstm(a_embeddings, (h_0, c_0))\n",
        "            # # print('lstm train')\n",
        "            # b_embeddings, (h_x, c_x) = self.lstm(b_embeddings, (h_0, c_0))\n",
        "        \"\"\"\n",
        "        input_a_word_embeddings = self.word_embeddings(input_ids_a)\n",
        "        input_b_word_embeddings = self.word_embeddings(input_ids_b)\n",
        "\n",
        "        a_embeddings = input_a_word_embeddings\n",
        "        a_embeddings = self.dropout(a_embeddings)\n",
        "\n",
        "        b_embeddings = input_b_word_embeddings\n",
        "        b_embeddings = self.dropout(b_embeddings)\n",
        "        return a_embeddings, b_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eygmVtDe-Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossAttLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query_a = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key_a = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value_a = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.query_b = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key_b = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value_b = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.dense = nn.Linear(config.hidden_size * 5, config.hidden_size)\n",
        "        self.norm = MatchGatNorm(config.hidden_size, eps=config.norm_eps)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        # x : batch_size * max_seq * dim\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)  \n",
        "        # new_x_shape: batch_size * max_seq * attention_heads * head_size\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # return shape: batch_size * attention_heads * max_seq * head_size\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b):\n",
        "        # hidden_states_a: batch_size * max_seq_a * embedding_dim\n",
        "        # hidden_states_b: batch_size * max_seq_b * embedding_dim\n",
        "\n",
        "        mixed_query_layer_a = self.query_a(hidden_states_a)\n",
        "        mixed_key_layer_a = self.key_a(hidden_states_a)\n",
        "        mixed_value_layer_a = self.value_a(hidden_states_a)\n",
        "\n",
        "        query_layer_a = self.transpose_for_scores(mixed_query_layer_a)\n",
        "        key_layer_a = self.transpose_for_scores(mixed_key_layer_a)\n",
        "        value_layer_a = self.transpose_for_scores(mixed_value_layer_a)\n",
        "\n",
        "        mixed_query_layer_b = self.query_b(hidden_states_b)\n",
        "        mixed_key_layer_b = self.key_b(hidden_states_b)\n",
        "        mixed_value_layer_b = self.value_b(hidden_states_b)\n",
        "\n",
        "        query_layer_b = self.transpose_for_scores(mixed_query_layer_b)\n",
        "        key_layer_b = self.transpose_for_scores(mixed_key_layer_b)\n",
        "        value_layer_b = self.transpose_for_scores(mixed_value_layer_b)\n",
        "\n",
        "        extended_attention_mask_a = attention_mask_a[:, None, None, :]\n",
        "        extended_attention_mask_a = (1.0 - extended_attention_mask_a) * -10000.0\n",
        "        attention_mask_a = extended_attention_mask_a\n",
        "\n",
        "        extended_attention_mask_b = attention_mask_b[:, None, None, :]\n",
        "        extended_attention_mask_b = (1.0 - extended_attention_mask_b) * -10000.0\n",
        "        attention_mask_b = extended_attention_mask_b\n",
        "\n",
        "\n",
        "        attention_scores_a2b = torch.matmul(query_layer_a, key_layer_b.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_a * max_seq_b\n",
        "        attention_scores_a2b = attention_scores_a2b / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_a2b = attention_scores_a2b + attention_mask_b\n",
        "        attention_probs_a2b = nn.Softmax(dim=-1)(attention_scores_a2b)\n",
        "        attention_probs_a2b = self.dropout(attention_probs_a2b)\n",
        "        context_layer_a2b = torch.matmul(attention_probs_a2b, value_layer_b)\n",
        "        context_layer_a2b = context_layer_a2b.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_a2b = context_layer_a2b.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_a2b = context_layer_a2b.view(*new_context_layer_shape_a2b)\n",
        "\n",
        "\n",
        "        \n",
        "        attention_scores_b2a = torch.matmul(query_layer_b, key_layer_a.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_b * max_seq_a\n",
        "        attention_scores_b2a = attention_scores_b2a / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_b2a = attention_scores_b2a + attention_mask_a\n",
        "        attention_probs_b2a = nn.Softmax(dim=-1)(attention_scores_b2a)\n",
        "        # attention_probs_b2a = self.dropout(attention_probs_b2a)\n",
        "        context_layer_b2a = torch.matmul(attention_probs_b2a, value_layer_a)\n",
        "        context_layer_b2a = context_layer_b2a.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_b2a = context_layer_b2a.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_b2a = context_layer_b2a.view(*new_context_layer_shape_b2a)\n",
        "\n",
        "        '''\n",
        "        context_layer_a = torch.cat([context_layer_a2a, context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([context_layer_a, context_layer_a2a - context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([context_layer_a, context_layer_a2a * context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([hidden_states_a, context_layer_a], -1)\n",
        "\n",
        "\n",
        "        context_layer_b = torch.cat([context_layer_b2b, context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([context_layer_b, context_layer_b2b - context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([context_layer_b, context_layer_b2b * context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([hidden_states_b, context_layer_b], -1)\n",
        "\n",
        "\n",
        "        # print('context_layer_a shape:', context_layer_a.shape)\n",
        "        # print('context_layer_b shape:', context_layer_b.shape)\n",
        "\n",
        "\n",
        "        context_layer_a = self.dense(context_layer_a)\n",
        "        context_layer_a = gelu(context_layer_a)\n",
        "\n",
        "        context_layer_b = self.dense(context_layer_b)\n",
        "        context_layer_b = gelu(context_layer_b)\n",
        "\n",
        "        context_layer_a = self.norm(hidden_states_a + context_layer_a)\n",
        "        context_layer_b = self.norm(hidden_states_b + context_layer_b)\n",
        "        '''\n",
        "\n",
        "        context_layer_a = self.norm(hidden_states_a + context_layer_a2b)\n",
        "        context_layer_b = self.norm(hidden_states_b + context_layer_b2a)\n",
        "        \n",
        "        outputs = (context_layer_a, context_layer_b)\n",
        "        \n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8w36YRh-ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"match-gat model\"\"\"\n",
        "\n",
        "class GatLayer(nn.Module):\n",
        "    \"\"\"docstring for GatLayer\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.align = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # self.align_a2b = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # self.align_b2a = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        self.dense = nn.Linear(config.hidden_size * 5, config.hidden_size)\n",
        "\n",
        "        self.norm = MatchGatNorm(config.hidden_size, eps=config.norm_eps)\n",
        "\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        # x : batch_size * max_seq * dim\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)  \n",
        "        # new_x_shape: batch_size * max_seq * attention_heads * head_size\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # return shape: batch_size * attention_heads * max_seq * head_size\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b):\n",
        "        # hidden_states_a: batch_size * max_seq_a * embedding_dim\n",
        "        # hidden_states_b: batch_size * max_seq_b * embedding_dim\n",
        "\n",
        "        mixed_query_layer_a = self.query(hidden_states_a)\n",
        "        mixed_key_layer_a = self.key(hidden_states_a)\n",
        "        mixed_value_layer_a = self.value(hidden_states_a)\n",
        "\n",
        "        query_layer_a = self.transpose_for_scores(mixed_query_layer_a)\n",
        "        key_layer_a = self.transpose_for_scores(mixed_key_layer_a)\n",
        "        value_layer_a = self.transpose_for_scores(mixed_value_layer_a)\n",
        "\n",
        "        mixed_query_layer_b = self.query(hidden_states_b)\n",
        "        mixed_key_layer_b = self.key(hidden_states_b)\n",
        "        mixed_value_layer_b = self.value(hidden_states_b)\n",
        "\n",
        "        query_layer_b = self.transpose_for_scores(mixed_query_layer_b)\n",
        "        key_layer_b = self.transpose_for_scores(mixed_key_layer_b)\n",
        "        value_layer_b = self.transpose_for_scores(mixed_value_layer_b)\n",
        "\n",
        "        extended_attention_mask_a = attention_mask_a[:, None, None, :]\n",
        "        extended_attention_mask_a = (1.0 - extended_attention_mask_a) * -10000.0\n",
        "        attention_mask_a = extended_attention_mask_a\n",
        "\n",
        "        extended_attention_mask_b = attention_mask_b[:, None, None, :]\n",
        "        extended_attention_mask_b = (1.0 - extended_attention_mask_b) * -10000.0\n",
        "        attention_mask_b = extended_attention_mask_b\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        attention_scores_a2b = torch.matmul(query_layer_a, key_layer_b.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_a * max_seq_b\n",
        "        attention_scores_a2b = attention_scores_a2b / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_a2b = attention_scores_a2b + attention_mask_b\n",
        "        attention_probs_a2b = nn.Softmax(dim=-1)(attention_scores_a2b)\n",
        "        attention_probs_a2b = self.dropout(attention_probs_a2b)\n",
        "        context_layer_a2b = torch.matmul(attention_probs_a2b, value_layer_b)\n",
        "        context_layer_a2b = context_layer_a2b.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_a2b = context_layer_a2b.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_a2b = context_layer_a2b.view(*new_context_layer_shape_a2b)\n",
        "\n",
        "\n",
        "        \n",
        "        attention_scores_b2a = torch.matmul(query_layer_b, key_layer_a.transpose(-1, -2)) \n",
        "        # batch_size * attention_heads * max_seq_b * max_seq_a\n",
        "        attention_scores_b2a = attention_scores_b2a / math.sqrt(self.attention_head_size)\n",
        "        attention_scores_b2a = attention_scores_b2a + attention_mask_a\n",
        "        attention_probs_b2a = nn.Softmax(dim=-1)(attention_scores_b2a)\n",
        "        # attention_probs_b2a = self.dropout(attention_probs_b2a)\n",
        "        context_layer_b2a = torch.matmul(attention_probs_b2a, value_layer_a)\n",
        "        context_layer_b2a = context_layer_b2a.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape_b2a = context_layer_b2a.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer_b2a = context_layer_b2a.view(*new_context_layer_shape_b2a)\n",
        "\n",
        "        '''\n",
        "        context_layer_a = torch.cat([context_layer_a2a, context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([context_layer_a, context_layer_a2a - context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([context_layer_a, context_layer_a2a * context_layer_a2b], -1)\n",
        "        context_layer_a = torch.cat([hidden_states_a, context_layer_a], -1)\n",
        "\n",
        "\n",
        "        context_layer_b = torch.cat([context_layer_b2b, context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([context_layer_b, context_layer_b2b - context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([context_layer_b, context_layer_b2b * context_layer_b2a], -1)\n",
        "        context_layer_b = torch.cat([hidden_states_b, context_layer_b], -1)\n",
        "\n",
        "\n",
        "        # print('context_layer_a shape:', context_layer_a.shape)\n",
        "        # print('context_layer_b shape:', context_layer_b.shape)\n",
        "\n",
        "\n",
        "        context_layer_a = self.dense(context_layer_a)\n",
        "        context_layer_a = gelu(context_layer_a)\n",
        "\n",
        "        context_layer_b = self.dense(context_layer_b)\n",
        "        context_layer_b = gelu(context_layer_b)\n",
        "\n",
        "        context_layer_a = self.norm(hidden_states_a + context_layer_a)\n",
        "        context_layer_b = self.norm(hidden_states_b + context_layer_b)\n",
        "        '''\n",
        "        context_layer_a = self.norm(hidden_states_a  + context_layer_a2b)\n",
        "        context_layer_b = self.norm(hidden_states_b  + context_layer_b2a)\n",
        "        \n",
        "        outputs = (context_layer_a, context_layer_b)\n",
        "        # outputs = (context_layer_a2b, context_layer_b2a)\n",
        "        \n",
        "        ### 拼接原来的a 与 对a对b做过attention的结果\n",
        "        \"\"\"\n",
        "        outputs_a = torch.cat([hidden_states_a, context_layer_a2b], dim = -1)\n",
        "        outputs_a = self.align(outputs_a)\n",
        "\n",
        "        outputs_b = torch.cat([hidden_states_b, context_layer_b2a], dim = -1)\n",
        "        outputs_b = self.align(outputs_b)\n",
        "\n",
        "        outputs = (outputs_a, outputs_b)\n",
        "        \"\"\"\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWbp692AiFMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embedding = EmbeddingLayer(config)\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.pooling = Pooling()\n",
        "        self.prediction = Prediction(config)\n",
        "\n",
        "        # self.init_weights()\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "         \n",
        "        \n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, MatchGatNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input_ids_a, input_ids_b, attention_mask_a, attention_mask_b, labels):\n",
        "        hidden_states_a, hidden_states_b = self.embedding(input_ids_a, input_ids_b)\n",
        "        for i, layer in enumerate(self.blocks):\n",
        "            hidden_states_a, hidden_states_b = layer(hidden_states_a, hidden_states_b, attention_mask_a, attention_mask_b)\n",
        "\n",
        "        # print('hidden_states_a:', hidden_states_a, hidden_states_a.shape)\n",
        "        # print('hidden_states_b:', hidden_states_b, hidden_states_b.shape)\n",
        "        # os._exit(1)\n",
        "\n",
        "        outputs_a = self.pooling(hidden_states_a, attention_mask_a)\n",
        "        outputs_b = self.pooling(hidden_states_b, attention_mask_b)\n",
        "\n",
        "        outputs = self.prediction(outputs_a, outputs_b)\n",
        "        # loss = self.loss_fct(outputs.view(-1, config.num_labels), labels.view(-1))\n",
        "        loss = self.loss_fct(outputs, labels)\n",
        "        # print(loss)\n",
        "        # print(outputs)\n",
        "        outputs = (loss, outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    \"\"\"\n",
        "    def init_weights(self, module):\n",
        "        # Initialize the weights \n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, norm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}